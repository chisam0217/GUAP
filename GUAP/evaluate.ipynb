{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from utils import load_data, accuracy, normalize, load_polblogs_data\n",
    "from models import GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    cuda = True\n",
    "    fastmode = False\n",
    "    seed = 20\n",
    "#     seed = 123\n",
    "    epochs = 200\n",
    "    lr = 0.01\n",
    "    weight_decay = 5e-4\n",
    "    hidden = 16\n",
    "    dropout = 0.5\n",
    "    pert_num = 20\n",
    "    L1 = 0.01\n",
    "    evaluate_mode = \"global_random\"\n",
    "    dataset = \"polblogs\"\n",
    "    radius = 4\n",
    "\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting 1 largest connected components\n",
      "33428.0\n",
      "(1222, 1222)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiao/anaconda3/envs/pytorch/lib/python3.6/site-packages/scipy/sparse/_index.py:112: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "if args.dataset == \"polblogs\":\n",
    "    tmp_adj, features, labels, idx_train, idx_test = load_polblogs_data()\n",
    "    print (sum(sum(tmp_adj)))\n",
    "    print (tmp_adj.shape)\n",
    "else:\n",
    "    _, features, labels, idx_train, idx_val, idx_test, tmp_adj  = load_data(args.dataset)\n",
    "\n",
    "num_classes = labels.max().item() + 1\n",
    "# tmp_adj = tmp_adj.toarray()\n",
    "\n",
    "adj = tmp_adj\n",
    "adj = np.eye(tmp_adj.shape[0]) + adj\n",
    "adj, _ = normalize(adj)\n",
    "adj = torch.from_numpy(adj.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and optimizer\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=args.hidden,\n",
    "            nclass=num_classes,\n",
    "            dropout=args.dropout)\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.cuda:\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    if args.dataset != \"polblogs\":\n",
    "        idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x = Variable(adj, requires_grad=True)\n",
    "    output = model(features, x)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "#     print ('output', output.size())\n",
    "#     print ('labels', labels.size())\n",
    "    loss_train.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    if args.dataset != \"polblogs\": \n",
    "        loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "        acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "        print('Epoch: {:04d}'.format(epoch+1),\n",
    "              'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "              'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "              'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "              'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "              'time: {:.4f}s'.format(time.time() - t))\n",
    "    else:\n",
    "        print('Epoch: {:04d}'.format(epoch+1),\n",
    "              'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "              'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "              'time: {:.4f}s'.format(time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(adj_m):\n",
    "    model.eval()\n",
    "    output = model(features, adj_m)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 0.6825 acc_train: 0.6694 time: 0.4668s\n",
      "Epoch: 0002 loss_train: 0.6769 acc_train: 0.7355 time: 0.0034s\n",
      "Epoch: 0003 loss_train: 0.6698 acc_train: 0.7769 time: 0.0041s\n",
      "Epoch: 0004 loss_train: 0.6644 acc_train: 0.6281 time: 0.0039s\n",
      "Epoch: 0005 loss_train: 0.6676 acc_train: 0.7355 time: 0.0034s\n",
      "Epoch: 0006 loss_train: 0.6382 acc_train: 0.8264 time: 0.0027s\n",
      "Epoch: 0007 loss_train: 0.6431 acc_train: 0.7686 time: 0.0031s\n",
      "Epoch: 0008 loss_train: 0.6060 acc_train: 0.8843 time: 0.0031s\n",
      "Epoch: 0009 loss_train: 0.6115 acc_train: 0.7686 time: 0.0028s\n",
      "Epoch: 0010 loss_train: 0.5864 acc_train: 0.9008 time: 0.0027s\n",
      "Epoch: 0011 loss_train: 0.5667 acc_train: 0.9174 time: 0.0030s\n",
      "Epoch: 0012 loss_train: 0.5501 acc_train: 0.9091 time: 0.0030s\n",
      "Epoch: 0013 loss_train: 0.5329 acc_train: 0.9669 time: 0.0030s\n",
      "Epoch: 0014 loss_train: 0.5235 acc_train: 0.9421 time: 0.0034s\n",
      "Epoch: 0015 loss_train: 0.4962 acc_train: 0.9339 time: 0.0027s\n",
      "Epoch: 0016 loss_train: 0.4642 acc_train: 0.9587 time: 0.0030s\n",
      "Epoch: 0017 loss_train: 0.4526 acc_train: 0.9752 time: 0.0031s\n",
      "Epoch: 0018 loss_train: 0.4374 acc_train: 0.9421 time: 0.0030s\n",
      "Epoch: 0019 loss_train: 0.4080 acc_train: 0.9752 time: 0.0029s\n",
      "Epoch: 0020 loss_train: 0.3983 acc_train: 0.9752 time: 0.0027s\n",
      "Epoch: 0021 loss_train: 0.3543 acc_train: 0.9752 time: 0.0030s\n",
      "Epoch: 0022 loss_train: 0.3485 acc_train: 0.9669 time: 0.0031s\n",
      "Epoch: 0023 loss_train: 0.3096 acc_train: 0.9752 time: 0.0031s\n",
      "Epoch: 0024 loss_train: 0.3074 acc_train: 0.9835 time: 0.0030s\n",
      "Epoch: 0025 loss_train: 0.2783 acc_train: 0.9835 time: 0.0030s\n",
      "Epoch: 0026 loss_train: 0.2689 acc_train: 0.9752 time: 0.0026s\n",
      "Epoch: 0027 loss_train: 0.2422 acc_train: 0.9835 time: 0.0026s\n",
      "Epoch: 0028 loss_train: 0.2510 acc_train: 0.9587 time: 0.0027s\n",
      "Epoch: 0029 loss_train: 0.2193 acc_train: 0.9587 time: 0.0031s\n",
      "Epoch: 0030 loss_train: 0.2016 acc_train: 0.9835 time: 0.0032s\n",
      "Epoch: 0031 loss_train: 0.1815 acc_train: 0.9669 time: 0.0027s\n",
      "Epoch: 0032 loss_train: 0.1852 acc_train: 0.9835 time: 0.0026s\n",
      "Epoch: 0033 loss_train: 0.1669 acc_train: 0.9917 time: 0.0030s\n",
      "Epoch: 0034 loss_train: 0.1656 acc_train: 0.9835 time: 0.0031s\n",
      "Epoch: 0035 loss_train: 0.1594 acc_train: 0.9752 time: 0.0031s\n",
      "Epoch: 0036 loss_train: 0.1596 acc_train: 0.9752 time: 0.0030s\n",
      "Epoch: 0037 loss_train: 0.1313 acc_train: 0.9917 time: 0.0026s\n",
      "Epoch: 0038 loss_train: 0.1412 acc_train: 0.9752 time: 0.0030s\n",
      "Epoch: 0039 loss_train: 0.1236 acc_train: 0.9917 time: 0.0031s\n",
      "Epoch: 0040 loss_train: 0.1227 acc_train: 0.9752 time: 0.0027s\n",
      "Epoch: 0041 loss_train: 0.1137 acc_train: 0.9835 time: 0.0026s\n",
      "Epoch: 0042 loss_train: 0.1168 acc_train: 0.9752 time: 0.0030s\n",
      "Epoch: 0043 loss_train: 0.1177 acc_train: 0.9835 time: 0.0030s\n",
      "Epoch: 0044 loss_train: 0.0972 acc_train: 0.9835 time: 0.0036s\n",
      "Epoch: 0045 loss_train: 0.1008 acc_train: 0.9835 time: 0.0033s\n",
      "Epoch: 0046 loss_train: 0.0986 acc_train: 0.9835 time: 0.0034s\n",
      "Epoch: 0047 loss_train: 0.0889 acc_train: 1.0000 time: 0.0032s\n",
      "Epoch: 0048 loss_train: 0.0927 acc_train: 1.0000 time: 0.0029s\n",
      "Epoch: 0049 loss_train: 0.0969 acc_train: 0.9917 time: 0.0033s\n",
      "Epoch: 0050 loss_train: 0.1006 acc_train: 0.9835 time: 0.0033s\n",
      "Epoch: 0051 loss_train: 0.0894 acc_train: 1.0000 time: 0.0033s\n",
      "Epoch: 0052 loss_train: 0.1056 acc_train: 0.9917 time: 0.0030s\n",
      "Epoch: 0053 loss_train: 0.0844 acc_train: 0.9917 time: 0.0030s\n",
      "Epoch: 0054 loss_train: 0.0747 acc_train: 0.9917 time: 0.0032s\n",
      "Epoch: 0055 loss_train: 0.0816 acc_train: 1.0000 time: 0.0029s\n",
      "Epoch: 0056 loss_train: 0.0833 acc_train: 0.9917 time: 0.0029s\n",
      "Epoch: 0057 loss_train: 0.0725 acc_train: 1.0000 time: 0.0034s\n",
      "Epoch: 0058 loss_train: 0.0819 acc_train: 0.9917 time: 0.0035s\n",
      "Epoch: 0059 loss_train: 0.0602 acc_train: 1.0000 time: 0.0030s\n",
      "Epoch: 0060 loss_train: 0.0738 acc_train: 0.9917 time: 0.0029s\n",
      "Epoch: 0061 loss_train: 0.0868 acc_train: 0.9835 time: 0.0027s\n",
      "Epoch: 0062 loss_train: 0.0827 acc_train: 0.9835 time: 0.0038s\n",
      "Epoch: 0063 loss_train: 0.0537 acc_train: 1.0000 time: 0.0029s\n",
      "Epoch: 0064 loss_train: 0.0655 acc_train: 1.0000 time: 0.0033s\n",
      "Epoch: 0065 loss_train: 0.0570 acc_train: 1.0000 time: 0.0028s\n",
      "Epoch: 0066 loss_train: 0.0572 acc_train: 0.9917 time: 0.0029s\n",
      "Epoch: 0067 loss_train: 0.0584 acc_train: 1.0000 time: 0.0028s\n",
      "Epoch: 0068 loss_train: 0.0599 acc_train: 1.0000 time: 0.0029s\n",
      "Epoch: 0069 loss_train: 0.0551 acc_train: 1.0000 time: 0.0028s\n",
      "Epoch: 0070 loss_train: 0.0584 acc_train: 0.9835 time: 0.0028s\n",
      "Epoch: 0071 loss_train: 0.0612 acc_train: 1.0000 time: 0.0028s\n",
      "Epoch: 0072 loss_train: 0.0540 acc_train: 1.0000 time: 0.0029s\n",
      "Epoch: 0073 loss_train: 0.0631 acc_train: 0.9917 time: 0.0028s\n",
      "Epoch: 0074 loss_train: 0.0687 acc_train: 0.9917 time: 0.0029s\n",
      "Epoch: 0075 loss_train: 0.0631 acc_train: 1.0000 time: 0.0028s\n",
      "Epoch: 0076 loss_train: 0.0783 acc_train: 0.9752 time: 0.0028s\n",
      "Epoch: 0077 loss_train: 0.0549 acc_train: 1.0000 time: 0.0032s\n",
      "Epoch: 0078 loss_train: 0.0483 acc_train: 1.0000 time: 0.0027s\n",
      "Epoch: 0079 loss_train: 0.0767 acc_train: 0.9917 time: 0.0032s\n",
      "Epoch: 0080 loss_train: 0.0483 acc_train: 1.0000 time: 0.0030s\n",
      "Epoch: 0081 loss_train: 0.0601 acc_train: 0.9917 time: 0.0032s\n",
      "Epoch: 0082 loss_train: 0.0653 acc_train: 1.0000 time: 0.0029s\n",
      "Epoch: 0083 loss_train: 0.0517 acc_train: 1.0000 time: 0.0033s\n",
      "Epoch: 0084 loss_train: 0.0531 acc_train: 1.0000 time: 0.0027s\n",
      "Epoch: 0085 loss_train: 0.0479 acc_train: 1.0000 time: 0.0028s\n",
      "Epoch: 0086 loss_train: 0.0447 acc_train: 1.0000 time: 0.0031s\n",
      "Epoch: 0087 loss_train: 0.0455 acc_train: 1.0000 time: 0.0032s\n",
      "Epoch: 0088 loss_train: 0.0472 acc_train: 1.0000 time: 0.0032s\n",
      "Epoch: 0089 loss_train: 0.0520 acc_train: 0.9917 time: 0.0032s\n",
      "Epoch: 0090 loss_train: 0.0401 acc_train: 1.0000 time: 0.0032s\n",
      "Epoch: 0091 loss_train: 0.0408 acc_train: 1.0000 time: 0.0031s\n",
      "Epoch: 0092 loss_train: 0.0526 acc_train: 1.0000 time: 0.0031s\n",
      "Epoch: 0093 loss_train: 0.0422 acc_train: 1.0000 time: 0.0041s\n",
      "Epoch: 0094 loss_train: 0.0412 acc_train: 1.0000 time: 0.0033s\n",
      "Epoch: 0095 loss_train: 0.0653 acc_train: 0.9835 time: 0.0032s\n",
      "Epoch: 0096 loss_train: 0.0458 acc_train: 1.0000 time: 0.0028s\n",
      "Epoch: 0097 loss_train: 0.0743 acc_train: 0.9917 time: 0.0030s\n",
      "Epoch: 0098 loss_train: 0.0498 acc_train: 1.0000 time: 0.0030s\n",
      "Epoch: 0099 loss_train: 0.0467 acc_train: 0.9917 time: 0.0031s\n",
      "Epoch: 0100 loss_train: 0.0399 acc_train: 1.0000 time: 0.0031s\n",
      "Epoch: 0101 loss_train: 0.0404 acc_train: 1.0000 time: 0.0031s\n",
      "Epoch: 0102 loss_train: 0.0348 acc_train: 1.0000 time: 0.0031s\n",
      "Epoch: 0103 loss_train: 0.0433 acc_train: 0.9917 time: 0.0031s\n",
      "Epoch: 0104 loss_train: 0.0378 acc_train: 1.0000 time: 0.0032s\n",
      "Epoch: 0105 loss_train: 0.0452 acc_train: 0.9917 time: 0.0032s\n",
      "Epoch: 0106 loss_train: 0.0403 acc_train: 1.0000 time: 0.0032s\n",
      "Epoch: 0107 loss_train: 0.0353 acc_train: 1.0000 time: 0.0031s\n",
      "Epoch: 0108 loss_train: 0.0353 acc_train: 1.0000 time: 0.0033s\n",
      "Epoch: 0109 loss_train: 0.0449 acc_train: 1.0000 time: 0.0032s\n",
      "Epoch: 0110 loss_train: 0.0377 acc_train: 0.9917 time: 0.0028s\n",
      "Epoch: 0111 loss_train: 0.0487 acc_train: 0.9917 time: 0.0027s\n",
      "Epoch: 0112 loss_train: 0.0426 acc_train: 0.9917 time: 0.0031s\n",
      "Epoch: 0113 loss_train: 0.0549 acc_train: 0.9917 time: 0.0030s\n",
      "Epoch: 0114 loss_train: 0.0487 acc_train: 0.9835 time: 0.0031s\n",
      "Epoch: 0115 loss_train: 0.0330 acc_train: 1.0000 time: 0.0031s\n",
      "Epoch: 0116 loss_train: 0.0314 acc_train: 1.0000 time: 0.0031s\n",
      "Epoch: 0117 loss_train: 0.0352 acc_train: 1.0000 time: 0.0032s\n",
      "Epoch: 0118 loss_train: 0.0444 acc_train: 1.0000 time: 0.0032s\n",
      "Epoch: 0119 loss_train: 0.0439 acc_train: 1.0000 time: 0.0037s\n",
      "Epoch: 0120 loss_train: 0.0404 acc_train: 1.0000 time: 0.0034s\n",
      "Epoch: 0121 loss_train: 0.0492 acc_train: 0.9917 time: 0.0032s\n",
      "Epoch: 0122 loss_train: 0.0438 acc_train: 1.0000 time: 0.0031s\n",
      "Epoch: 0123 loss_train: 0.0427 acc_train: 1.0000 time: 0.0030s\n",
      "Epoch: 0124 loss_train: 0.0353 acc_train: 1.0000 time: 0.0028s\n",
      "Epoch: 0125 loss_train: 0.0478 acc_train: 1.0000 time: 0.0031s\n",
      "Epoch: 0126 loss_train: 0.0385 acc_train: 1.0000 time: 0.0027s\n",
      "Epoch: 0127 loss_train: 0.0335 acc_train: 1.0000 time: 0.0031s\n",
      "Epoch: 0128 loss_train: 0.0342 acc_train: 1.0000 time: 0.0031s\n",
      "Epoch: 0129 loss_train: 0.0369 acc_train: 1.0000 time: 0.0031s\n",
      "Epoch: 0130 loss_train: 0.0341 acc_train: 1.0000 time: 0.0028s\n",
      "Epoch: 0131 loss_train: 0.0304 acc_train: 1.0000 time: 0.0033s\n",
      "Epoch: 0132 loss_train: 0.0322 acc_train: 1.0000 time: 0.0036s\n",
      "Epoch: 0133 loss_train: 0.0356 acc_train: 1.0000 time: 0.0033s\n",
      "Epoch: 0134 loss_train: 0.0311 acc_train: 1.0000 time: 0.0029s\n",
      "Epoch: 0135 loss_train: 0.0461 acc_train: 0.9835 time: 0.0027s\n",
      "Epoch: 0136 loss_train: 0.0411 acc_train: 0.9917 time: 0.0027s\n",
      "Epoch: 0137 loss_train: 0.0372 acc_train: 1.0000 time: 0.0026s\n",
      "Epoch: 0138 loss_train: 0.0294 acc_train: 1.0000 time: 0.0026s\n",
      "Epoch: 0139 loss_train: 0.0339 acc_train: 1.0000 time: 0.0031s\n",
      "Epoch: 0140 loss_train: 0.0430 acc_train: 1.0000 time: 0.0031s\n",
      "Epoch: 0141 loss_train: 0.0441 acc_train: 0.9917 time: 0.0031s\n",
      "Epoch: 0142 loss_train: 0.0313 acc_train: 1.0000 time: 0.0031s\n",
      "Epoch: 0143 loss_train: 0.0294 acc_train: 1.0000 time: 0.0032s\n",
      "Epoch: 0144 loss_train: 0.0413 acc_train: 0.9917 time: 0.0032s\n",
      "Epoch: 0145 loss_train: 0.0369 acc_train: 0.9917 time: 0.0036s\n",
      "Epoch: 0146 loss_train: 0.0388 acc_train: 1.0000 time: 0.0030s\n",
      "Epoch: 0147 loss_train: 0.0283 acc_train: 1.0000 time: 0.0034s\n",
      "Epoch: 0148 loss_train: 0.0382 acc_train: 1.0000 time: 0.0026s\n",
      "Epoch: 0149 loss_train: 0.0313 acc_train: 1.0000 time: 0.0027s\n",
      "Epoch: 0150 loss_train: 0.0413 acc_train: 1.0000 time: 0.0032s\n",
      "Epoch: 0151 loss_train: 0.0306 acc_train: 1.0000 time: 0.0028s\n",
      "Epoch: 0152 loss_train: 0.0324 acc_train: 1.0000 time: 0.0027s\n",
      "Epoch: 0153 loss_train: 0.0250 acc_train: 1.0000 time: 0.0027s\n",
      "Epoch: 0154 loss_train: 0.0360 acc_train: 1.0000 time: 0.0031s\n",
      "Epoch: 0155 loss_train: 0.0283 acc_train: 1.0000 time: 0.0027s\n",
      "Epoch: 0156 loss_train: 0.0297 acc_train: 1.0000 time: 0.0032s\n",
      "Epoch: 0157 loss_train: 0.0331 acc_train: 1.0000 time: 0.0029s\n",
      "Epoch: 0158 loss_train: 0.0316 acc_train: 1.0000 time: 0.0030s\n",
      "Epoch: 0159 loss_train: 0.0394 acc_train: 1.0000 time: 0.0030s\n",
      "Epoch: 0160 loss_train: 0.0332 acc_train: 1.0000 time: 0.0033s\n",
      "Epoch: 0161 loss_train: 0.0460 acc_train: 0.9835 time: 0.0033s\n",
      "Epoch: 0162 loss_train: 0.0454 acc_train: 0.9835 time: 0.0032s\n",
      "Epoch: 0163 loss_train: 0.0300 acc_train: 1.0000 time: 0.0029s\n",
      "Epoch: 0164 loss_train: 0.0336 acc_train: 1.0000 time: 0.0028s\n",
      "Epoch: 0165 loss_train: 0.0343 acc_train: 0.9917 time: 0.0033s\n",
      "Epoch: 0166 loss_train: 0.0269 acc_train: 1.0000 time: 0.0027s\n",
      "Epoch: 0167 loss_train: 0.0371 acc_train: 1.0000 time: 0.0031s\n",
      "Epoch: 0168 loss_train: 0.0401 acc_train: 1.0000 time: 0.0031s\n",
      "Epoch: 0169 loss_train: 0.0383 acc_train: 1.0000 time: 0.0033s\n",
      "Epoch: 0170 loss_train: 0.0296 acc_train: 1.0000 time: 0.0030s\n",
      "Epoch: 0171 loss_train: 0.0370 acc_train: 1.0000 time: 0.0033s\n",
      "Epoch: 0172 loss_train: 0.0281 acc_train: 1.0000 time: 0.0029s\n",
      "Epoch: 0173 loss_train: 0.0297 acc_train: 0.9917 time: 0.0032s\n",
      "Epoch: 0174 loss_train: 0.0304 acc_train: 1.0000 time: 0.0032s\n",
      "Epoch: 0175 loss_train: 0.0326 acc_train: 1.0000 time: 0.0026s\n",
      "Epoch: 0176 loss_train: 0.0302 acc_train: 1.0000 time: 0.0032s\n",
      "Epoch: 0177 loss_train: 0.0307 acc_train: 1.0000 time: 0.0032s\n",
      "Epoch: 0178 loss_train: 0.0426 acc_train: 0.9917 time: 0.0032s\n",
      "Epoch: 0179 loss_train: 0.0328 acc_train: 0.9917 time: 0.0029s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0180 loss_train: 0.0267 acc_train: 1.0000 time: 0.0037s\n",
      "Epoch: 0181 loss_train: 0.0352 acc_train: 1.0000 time: 0.0032s\n",
      "Epoch: 0182 loss_train: 0.0283 acc_train: 1.0000 time: 0.0032s\n",
      "Epoch: 0183 loss_train: 0.0440 acc_train: 0.9917 time: 0.0039s\n",
      "Epoch: 0184 loss_train: 0.0373 acc_train: 0.9917 time: 0.0036s\n",
      "Epoch: 0185 loss_train: 0.0678 acc_train: 0.9917 time: 0.0030s\n",
      "Epoch: 0186 loss_train: 0.0358 acc_train: 1.0000 time: 0.0028s\n",
      "Epoch: 0187 loss_train: 0.0370 acc_train: 1.0000 time: 0.0028s\n",
      "Epoch: 0188 loss_train: 0.0309 acc_train: 1.0000 time: 0.0028s\n",
      "Epoch: 0189 loss_train: 0.0290 acc_train: 1.0000 time: 0.0028s\n",
      "Epoch: 0190 loss_train: 0.0250 acc_train: 1.0000 time: 0.0028s\n",
      "Epoch: 0191 loss_train: 0.0211 acc_train: 1.0000 time: 0.0027s\n",
      "Epoch: 0192 loss_train: 0.0252 acc_train: 1.0000 time: 0.0031s\n",
      "Epoch: 0193 loss_train: 0.0247 acc_train: 1.0000 time: 0.0028s\n",
      "Epoch: 0194 loss_train: 0.0310 acc_train: 1.0000 time: 0.0028s\n",
      "Epoch: 0195 loss_train: 0.0229 acc_train: 1.0000 time: 0.0031s\n",
      "Epoch: 0196 loss_train: 0.0301 acc_train: 1.0000 time: 0.0040s\n",
      "Epoch: 0197 loss_train: 0.0454 acc_train: 0.9917 time: 0.0033s\n",
      "Epoch: 0198 loss_train: 0.0240 acc_train: 1.0000 time: 0.0035s\n",
      "Epoch: 0199 loss_train: 0.0340 acc_train: 1.0000 time: 0.0028s\n",
      "Epoch: 0200 loss_train: 0.0250 acc_train: 1.0000 time: 0.0032s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 1.1393s\n",
      "Test set results: loss= 0.1569 accuracy= 0.9500\n"
     ]
    }
   ],
   "source": [
    "t_total = time.time()\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# torch.save(model, './cora_gcn.pth')\n",
    "# torch.save(model.state_dict(), 'cora_gcn.pkl')\n",
    "\n",
    "# Testing\n",
    "ori_output = test(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_perturb(input_adj, idx, perturb):\n",
    "    # (1-x)A + x(1-A)\n",
    "#     input_adj = input_adj.toarray()\n",
    "\n",
    "    x = np.zeros((input_adj.shape[0], input_adj.shape[1]))\n",
    "    x[idx] = perturb  \n",
    "    x[:,idx] = perturb\n",
    "#     print ('x', x[idx])\n",
    "\n",
    "    \n",
    "#     x += np.transpose(x) #change the idx'th row and column\n",
    "    x1 = np.ones((input_adj.shape[0], input_adj.shape[1])) - x\n",
    "#     print ('x1', x1[idx])\n",
    "    adj2 = np.ones((input_adj.shape[0], input_adj.shape[1])) - input_adj\n",
    "#     print ('adj2', adj2[idx])\n",
    "\n",
    "    for i in range(input_adj.shape[0]):      \n",
    "        adj2[i][i] = 0\n",
    "\n",
    "    perturbed_adj = np.multiply(x1, input_adj) + np.multiply(x, adj2)\n",
    "    return perturbed_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_attack(perturb):\n",
    "    res = []\n",
    "    # perturb = np.where(perturb>0.5, 1, 0)\n",
    "    print ('perturb', perturb)\n",
    "    new_pred = []\n",
    "    for i in range(num_classes):\n",
    "        new_pred.append(0)\n",
    "    for k in idx_test:\n",
    "#     for k in range(1):\n",
    "#         print ('test node', k)\n",
    "        innormal_x_p = add_perturb(tmp_adj, k, perturb)\n",
    "#         print ('the perturbed conn', sum(innormal_x_p[k]))\n",
    "#         innormal_x_p = np.where(innormal_x_p<0.5, 0, 1)\n",
    "\n",
    "#         diff = innormal_x_p[k] - tmp_adj[k]\n",
    "#         diff_idx = np.where(diff != 0 )\n",
    "        \n",
    "#         print ('diff_idx', diff_idx)\n",
    "    #     one_idx = np.where(innormal_x_p[k]==1)[0]\n",
    "    #     zero_idx = np.where(innormal_x_p[k]!=1)[0]\n",
    "    #     total_idx = one_idx.shape[0] + zero_idx.shape[0]\n",
    "    #     print ('total_idx', total_idx)\n",
    "    #     print ('one_idx', one_idx)\n",
    "    #     print ('corresponding perturb', perturb[one_idx])\n",
    "    #     print (innormal_x_p[k][one_idx])\n",
    "        x_p, degree_p = normalize(innormal_x_p + np.eye(tmp_adj.shape[0]))\n",
    "        x_p = torch.from_numpy(x_p.astype(np.float32))\n",
    "        x_p = x_p.cuda()\n",
    "        output = model(features, x_p)\n",
    "        new_pred[int(torch.argmax(output[k]))] += 1\n",
    "        if int(torch.argmax(output[k])) == int(torch.argmax(ori_output[k])):\n",
    "            res.append(0)\n",
    "            print ('node {} attack failed'.format(k))\n",
    "        else:\n",
    "            res.append(1)\n",
    "            print ('node {} attack succeed'.format(k))\n",
    "    fooling_rate = float(sum(res)/len(res))\n",
    "    print ('the current fooling rate is', fooling_rate)\n",
    "    return fooling_rate, new_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(pred):\n",
    "    h = 0\n",
    "    all_pred = sum(pred)\n",
    "    for i in range(num_classes):\n",
    "        Pi = pred[i]/all_pred\n",
    "        if Pi != 0:\n",
    "            h -=  Pi* math.log(Pi)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the entropy is 0.6894199801171881\n"
     ]
    }
   ],
   "source": [
    "new_pred = []\n",
    "for i in range(num_classes):\n",
    "    new_pred.append(0)\n",
    "for k in idx_test:\n",
    "    new_pred[int(torch.argmax(ori_output[k]))] += 1\n",
    "entropy = calculate_entropy(new_pred)\n",
    "print ('the entropy is', entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the prob is 0.004909983633387889\n",
      "the perturbation is [330 547 609 649 950 955 972 995]\n",
      "perturb [0 0 0 ... 0 0 0]\n",
      "node 0 attack failed\n",
      "node 1 attack failed\n",
      "node 2 attack failed\n",
      "node 3 attack succeed\n",
      "node 4 attack succeed\n",
      "node 5 attack failed\n",
      "node 6 attack failed\n",
      "node 7 attack failed\n",
      "node 10 attack failed\n",
      "node 11 attack failed\n",
      "node 12 attack failed\n",
      "node 14 attack succeed\n",
      "node 15 attack failed\n",
      "node 16 attack succeed\n",
      "node 17 attack failed\n",
      "node 18 attack failed\n",
      "node 19 attack failed\n",
      "node 20 attack failed\n",
      "node 21 attack failed\n",
      "node 22 attack succeed\n",
      "node 23 attack failed\n",
      "node 24 attack failed\n",
      "node 26 attack succeed\n",
      "node 27 attack failed\n",
      "node 28 attack succeed\n",
      "node 29 attack succeed\n",
      "node 30 attack succeed\n",
      "node 31 attack failed\n",
      "node 32 attack failed\n",
      "node 33 attack succeed\n",
      "node 34 attack succeed\n",
      "node 35 attack failed\n",
      "node 36 attack failed\n",
      "node 37 attack failed\n",
      "node 38 attack failed\n",
      "node 39 attack succeed\n",
      "node 41 attack failed\n",
      "node 42 attack failed\n",
      "node 43 attack succeed\n",
      "node 44 attack succeed\n",
      "node 45 attack succeed\n",
      "node 46 attack failed\n",
      "node 47 attack failed\n",
      "node 48 attack failed\n",
      "node 49 attack succeed\n",
      "node 50 attack succeed\n",
      "node 51 attack succeed\n",
      "node 52 attack succeed\n",
      "node 53 attack failed\n",
      "node 55 attack failed\n",
      "node 56 attack failed\n",
      "node 57 attack succeed\n",
      "node 59 attack failed\n",
      "node 60 attack failed\n",
      "node 61 attack failed\n",
      "node 62 attack succeed\n",
      "node 63 attack succeed\n",
      "node 64 attack failed\n",
      "node 65 attack failed\n",
      "node 66 attack failed\n",
      "node 67 attack succeed\n",
      "node 68 attack failed\n",
      "node 69 attack succeed\n",
      "node 70 attack succeed\n",
      "node 72 attack succeed\n",
      "node 73 attack failed\n",
      "node 74 attack succeed\n",
      "node 75 attack failed\n",
      "node 76 attack failed\n",
      "node 77 attack succeed\n",
      "node 78 attack succeed\n",
      "node 79 attack failed\n",
      "node 80 attack failed\n",
      "node 81 attack failed\n",
      "node 82 attack succeed\n",
      "node 83 attack failed\n",
      "node 84 attack failed\n",
      "node 85 attack failed\n",
      "node 86 attack failed\n",
      "node 87 attack succeed\n",
      "node 88 attack succeed\n",
      "node 91 attack succeed\n",
      "node 92 attack failed\n",
      "node 93 attack succeed\n",
      "node 94 attack failed\n",
      "node 95 attack failed\n",
      "node 96 attack failed\n",
      "node 97 attack succeed\n",
      "node 98 attack failed\n",
      "node 99 attack failed\n",
      "node 100 attack failed\n",
      "node 101 attack failed\n",
      "node 102 attack failed\n",
      "node 103 attack failed\n",
      "node 104 attack failed\n",
      "node 105 attack failed\n",
      "node 106 attack succeed\n",
      "node 107 attack succeed\n",
      "node 108 attack failed\n",
      "node 109 attack failed\n",
      "node 110 attack succeed\n",
      "node 111 attack failed\n",
      "node 112 attack failed\n",
      "node 113 attack failed\n",
      "node 114 attack failed\n",
      "node 116 attack succeed\n",
      "node 117 attack failed\n",
      "node 118 attack failed\n",
      "node 119 attack failed\n",
      "node 120 attack succeed\n",
      "node 121 attack failed\n",
      "node 122 attack failed\n",
      "node 123 attack failed\n",
      "node 127 attack failed\n",
      "node 128 attack failed\n",
      "node 129 attack succeed\n",
      "node 130 attack failed\n",
      "node 131 attack failed\n",
      "node 132 attack failed\n",
      "node 133 attack succeed\n",
      "node 134 attack failed\n",
      "node 136 attack failed\n",
      "node 137 attack failed\n",
      "node 138 attack failed\n",
      "node 139 attack succeed\n",
      "node 140 attack failed\n",
      "node 141 attack failed\n",
      "node 142 attack succeed\n",
      "node 143 attack failed\n",
      "node 144 attack failed\n",
      "node 145 attack succeed\n",
      "node 146 attack failed\n",
      "node 147 attack failed\n",
      "node 148 attack succeed\n",
      "node 149 attack succeed\n",
      "node 150 attack failed\n",
      "node 151 attack failed\n",
      "node 152 attack failed\n",
      "node 153 attack failed\n",
      "node 154 attack failed\n",
      "node 155 attack failed\n",
      "node 156 attack failed\n",
      "node 157 attack succeed\n",
      "node 158 attack failed\n",
      "node 159 attack succeed\n",
      "node 160 attack succeed\n",
      "node 161 attack succeed\n",
      "node 162 attack failed\n",
      "node 163 attack failed\n",
      "node 164 attack failed\n",
      "node 165 attack failed\n",
      "node 166 attack failed\n",
      "node 167 attack failed\n",
      "node 168 attack failed\n",
      "node 169 attack failed\n",
      "node 170 attack failed\n",
      "node 171 attack succeed\n",
      "node 172 attack failed\n",
      "node 173 attack succeed\n",
      "node 174 attack failed\n",
      "node 175 attack failed\n",
      "node 176 attack failed\n",
      "node 177 attack failed\n",
      "node 178 attack failed\n",
      "node 179 attack succeed\n",
      "node 180 attack failed\n",
      "node 181 attack failed\n",
      "node 182 attack succeed\n",
      "node 183 attack failed\n",
      "node 184 attack failed\n",
      "node 185 attack failed\n",
      "node 186 attack failed\n",
      "node 187 attack succeed\n",
      "node 188 attack succeed\n",
      "node 189 attack failed\n",
      "node 191 attack failed\n",
      "node 192 attack failed\n",
      "node 193 attack succeed\n",
      "node 194 attack failed\n",
      "node 195 attack succeed\n",
      "node 196 attack failed\n",
      "node 197 attack failed\n",
      "node 198 attack succeed\n",
      "node 199 attack failed\n",
      "node 200 attack failed\n",
      "node 201 attack succeed\n",
      "node 202 attack failed\n",
      "node 203 attack failed\n",
      "node 205 attack failed\n",
      "node 206 attack failed\n",
      "node 207 attack failed\n",
      "node 208 attack failed\n",
      "node 209 attack succeed\n",
      "node 210 attack succeed\n",
      "node 212 attack succeed\n",
      "node 213 attack failed\n",
      "node 214 attack failed\n",
      "node 215 attack failed\n",
      "node 216 attack failed\n",
      "node 217 attack succeed\n",
      "node 219 attack failed\n",
      "node 220 attack failed\n",
      "node 221 attack failed\n",
      "node 223 attack succeed\n",
      "node 224 attack failed\n",
      "node 225 attack failed\n",
      "node 226 attack failed\n",
      "node 227 attack succeed\n",
      "node 228 attack failed\n",
      "node 229 attack failed\n",
      "node 230 attack failed\n",
      "node 231 attack failed\n",
      "node 232 attack failed\n",
      "node 233 attack failed\n",
      "node 234 attack failed\n",
      "node 235 attack failed\n",
      "node 236 attack failed\n",
      "node 237 attack failed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-6eb0190fe381>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperturb\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'the perturbation is'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_attack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperturb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mfool_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-09232120dcf8>\u001b[0m in \u001b[0;36mevaluate_attack\u001b[0;34m(perturb)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#     for k in range(1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#         print ('test node', k)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0minnormal_x_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_perturb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_adj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperturb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m#         print ('the perturbed conn', sum(innormal_x_p[k]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#         innormal_x_p = np.where(innormal_x_p<0.5, 0, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-dc7cd1ac3ec3>\u001b[0m in \u001b[0;36madd_perturb\u001b[0;34m(input_adj, idx, perturb)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_adj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_adj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#     print ('x1', x1[idx])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0madj2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_adj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_adj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0minput_adj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;31m#     print ('adj2', adj2[idx])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mones\u001b[0;34m(shape, dtype, order)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \"\"\"\n\u001b[1;32m    223\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0mmultiarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'unsafe'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#evaluate the universal attack\n",
    "if args.evaluate_mode == \"universal\":\n",
    "    fool_res = []\n",
    "    p_times = []\n",
    "    all_entropy = []\n",
    "    for i in range(10):\n",
    "        perturb = np.array([float(line.rstrip('\\n')) for line in open('./perturbation_results/{1}_xi{2}_epoch100/perturbation_{1}_{0}.txt'.format(i, args.dataset, args.radius))])\n",
    "        perturb = np.where(perturb>0.5, 1, 0)\n",
    "        pt = np.where(perturb>0)[0]\n",
    "        if len(list(pt)) == 0:\n",
    "            fool_res.append(0)\n",
    "            p_times.append(0)\n",
    "            continue\n",
    "        print ('the perturbation is', pt)\n",
    "        res, new_pred = evaluate_attack(perturb)\n",
    "        print ('the prediction result is', new_pred)\n",
    "        entropy = calculate_entropy(new_pred)\n",
    "        fool_res.append(res)      \n",
    "        p_times.append(len(list(pt)))\n",
    "        print ('the perturbation times is', p_times)\n",
    "        print ('the fooling rates are', fool_res)\n",
    "        print ('the average fooling rates over 10 times of test is', sum(fool_res)/float(len(fool_res)))\n",
    "        print ('the entropy is', entropy)\n",
    "        all_entropy.append(entropy)\n",
    "    print ('all the entropy values are', all_entropy)\n",
    "    print ('the average entropy is', sum(all_entropy)/float(len(all_entropy)))\n",
    "        \n",
    "elif args.evaluate_mode == \"global_random\":\n",
    "    #set this equal to the ceil of the number of anchor nodes computed by universal attack\n",
    "    perturb_times = 6\n",
    "    fool_res = []\n",
    "    p_times = []\n",
    "    for i in range(10):\n",
    "#         perturb = np.array([float(line.rstrip('\\n')) for line in open(\"perturbation.txt\")])\n",
    "#         perturb = np.where(perturb>0.5, 1, 0)\n",
    "#         perturb_times = sum(perturb)\n",
    "        # perturb = np.zeros(adj.shape[1])\n",
    "        #the perturbation times of our universal perturbation\n",
    "        # attack_index = list(np.random.choice(range(adj.shape[1]), perturb_times, replace = False))\n",
    "        # perturb[attack_index] = 1\n",
    "        prob = float(perturb_times / tmp_adj.shape[0])\n",
    "        perturb = np.random.choice(2, tmp_adj.shape[0], p = [1-prob, prob])\n",
    "        print ('the prob is', prob)\n",
    "        pt = np.where(perturb>0)[0]\n",
    "        print ('the perturbation is', pt)\n",
    "        res, new_pred = evaluate_attack(perturb)\n",
    "        fool_res.append(res)\n",
    "\n",
    "        p_times.append(len(list(pt)))\n",
    "        print ('the perturbation times is', p_times)\n",
    "        print ('the fooling rates are', fool_res)\n",
    "        print ('the average fooling rates over 10 times of test is', sum(fool_res)/float(len(fool_res)))\n",
    "\n",
    "    print ('the average fooling rate with {} perturbation times is'.format(perturb_times), sum(fool_res)/float(len(fool_res)))\n",
    "    \n",
    "elif args.evaluate_mode == \"limitted_random\":\n",
    "    \n",
    "    perturb_times = 8\n",
    "    fool_res = []\n",
    "    p_times = []\n",
    "    for i in range(10):\n",
    "#         perturb = np.array([float(line.rstrip('\\n')) for line in open(\"perturbation.txt\")])\n",
    "#         perturb = np.where(perturb>0.5, 1, 0)\n",
    "#         perturb_times = sum(perturb)\n",
    "        perturb = np.zeros(adj.shape[1])\n",
    "        #the perturbation times of our universal perturbation\n",
    "        attack_index = list(np.random.choice(range(adj.shape[1]), perturb_times, replace = False))\n",
    "        perturb[attack_index] = 1\n",
    "        pt = np.where(perturb>0)[0]\n",
    "#         print ('the perturbation is', pt)\n",
    "        res, new_pred = evaluate_attack(perturb)\n",
    "        fool_res.append(res)\n",
    "\n",
    "        p_times.append(len(list(pt)))\n",
    "        print ('the perturbation times is', p_times)\n",
    "        print ('the fooling rates are', fool_res)\n",
    "        print ('the average fooling rates over 10 times of test is', sum(fool_res)/float(len(fool_res)))\n",
    "\n",
    "    print ('the average fooling rate with {} perturbation times is'.format(perturb_times), sum(fool_res)/float(len(fool_res)))\n",
    "    \n",
    "elif args.evaluate_mode == \"victim_attak\":\n",
    "    #the perturbation times of our universal perturbation\n",
    "    perturb_time = 8 #set this equal to the ceil of the number of anchor nodes computed by universal attack\n",
    "    fool_res = []\n",
    "#     p_times = []\n",
    "    for k in range(num_classes):\n",
    "#     for k in range(4,6):\n",
    "        each_fool_res = []\n",
    "        idx = np.where(labels.cpu().numpy()==k)[0]\n",
    "#         for i in range(1):\n",
    "        for i in range(10):\n",
    "            attack_index = list(np.random.choice(idx, perturb_time, replace = False))\n",
    "            perturb = np.zeros(adj.shape[1])\n",
    "            perturb[attack_index] = 1\n",
    "            print ('perturbating by connecting to nodes of class', k)\n",
    "            res, new_pred = evaluate_attack(perturb)\n",
    "            each_fool_res.append(res)\n",
    "            print ('the fooling rates of current class are', each_fool_res)\n",
    "            avg_asr = sum(each_fool_res)/float(len(each_fool_res))\n",
    "            print ('the average fooling rates over 10 times of test is', avg_asr)\n",
    "        fool_res.append(avg_asr)\n",
    "        print ('fool_res', fool_res)\n",
    "    print ('the avg asr by connecting to each class of nodes is', fool_res)\n",
    "        \n",
    "elif args.evaluate_mode == \"universal_delete\":   \n",
    "    \n",
    "    all_fool = []\n",
    "    for i in range(8, 9):\n",
    "        fool_res = []\n",
    "        for j in range(8):\n",
    "            perturb = np.array([float(line.rstrip('\\n')) for line in open('./perturbation_results/{1}_xi{2}_epoch100/perturbation_{1}_4.txt'.format(i, args.dataset, args.radius))])\n",
    "            perturb = np.where(perturb>0.5, 1, 0)\n",
    "            pt = np.where(perturb>0)[0]\n",
    "            a = list(np.random.choice(range(0, pt.shape[0]), i, replace = False))\n",
    "            perturb[pt[a]] = 0\n",
    "            pt = np.where(perturb>0)[0]\n",
    "            print ('the perturbation is', pt)\n",
    "            res, new_pred = evaluate_attack(perturb)\n",
    "            fool_res.append(res)      \n",
    "            print ('the fooling rates are', fool_res)\n",
    "            print ('the average fooling rates over 10 times of test is', sum(fool_res)/float(len(fool_res)))\n",
    "        all_fool.append(sum(fool_res)/float(len(fool_res)))\n",
    "          \n",
    "    print ('all the fooling rate is', all_fool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
